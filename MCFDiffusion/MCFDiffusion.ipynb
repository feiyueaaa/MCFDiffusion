{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b1262-063f-4ed1-9d66-911e0f8e365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logger\n",
    "import fp16_util\n",
    "import unet\n",
    "import gaussian_diffusion\n",
    "import script_util\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda:0')\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9180bb-f7b4-491d-8199-058866cbfd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('unet/model/path')\n",
    "classifier = torch.load('classifier/model/path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d0cff-03ea-4bd6-b063-bf52a34da27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.Resize((256,256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a804781-26fd-40e5-8662-bbe1765a3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def betas_cos(n_steps,max_beta=0.999):\n",
    "        # Cosine Noise Scheme Generation Beta\n",
    "        betas = []\n",
    "        alpha_bar = lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2\n",
    "        for i in range(n_steps):\n",
    "            t1 = i / n_steps\n",
    "            t2 = (i + 1) / n_steps\n",
    "            betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "        return torch.Tensor(betas)\n",
    "\n",
    "\n",
    "\n",
    "# Retrieve the data with index t from `vals`, and reshape it into the required format.\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "\n",
    "\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=device):\n",
    "\n",
    "\n",
    "    left_img,middle_img,right_img = torch.chunk(x_0,chunks=3,dim=1)\n",
    "\n",
    "    noise = torch.randn_like(middle_img)\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, middle_img.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, middle_img.shape\n",
    "    )\n",
    "    return sqrt_alphas_cumprod_t.to(device) * middle_img.to(device) \\\n",
    "    + 0.5*sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "\n",
    "T = 1000\n",
    "beta = betas_cos(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78436a-db1c-42f5-b301-12371bfe7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 1. - beta\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = beta * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "eta = 0\n",
    "\n",
    "def get_sigma(eta):\n",
    "    sigma = (\n",
    "        \n",
    "            eta\n",
    "            * torch.sqrt((1 - alphas_cumprod_prev) / (1 - alphas_cumprod))\n",
    "            * torch.sqrt(1 - alphas_cumprod / alphas_cumprod_prev)\n",
    "        )\n",
    "\n",
    "    return sigma.to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793a30f-906c-4053-9900-839e1a13e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the tensor to an image.\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :] \n",
    "    plt.imshow(reverse_transforms(image.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7eb11-542f-47a8-8332-6aed8fd2c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classfier_guidence\n",
    "def cond_fn(x, y=1,t=None,s=0):\n",
    "    assert y is not None\n",
    "    with torch.enable_grad():\n",
    "        x_in = x.detach().requires_grad_(True)\n",
    "        logits = classifier(x_in,t)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        selected = log_probs[range(len(logits)), y.view(-1)]\n",
    "        return torch.autograd.grad(selected.sum(), x_in)[0] * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e0510-caf3-47f0-bfec-94749ee570c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(eta, x, t,y=None):\n",
    "\n",
    "    noise_pred = model(x, t,y)\n",
    "    left_img,x,right_img = torch.chunk(x,chunks=3,dim=1)\n",
    "    img = trans_pil(left_img[0])\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    ).to(device)\n",
    "\n",
    "    alphas_comprod_t = get_index_from_list(alphas_cumprod,t,x.shape).to(device)\n",
    "    alphas_comprod_prev_t = get_index_from_list(alphas_cumprod_prev,t,x.shape).to(device)\n",
    "    sigma = get_sigma(eta).to(device)\n",
    "    sigma_t =  get_index_from_list(sigma.cpu(), t, x.shape).to(device)\n",
    "    \n",
    "\n",
    "    x.to(device)\n",
    "\n",
    "    prev_x = torch.sqrt(alphas_comprod_prev_t) * (\n",
    "        (x.to(device) - torch.sqrt(1 - alphas_comprod_t) * noise_pred) /torch.sqrt(alphas_comprod_t)\n",
    "     ) + torch.sqrt(1- alphas_comprod_prev_t - sigma_t**2)*noise_pred \n",
    "    \n",
    "    \n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    if t == 0:\n",
    "        return prev_x\n",
    "    else:\n",
    "        noise = torch.randn_like(x).to(device)\n",
    "        return prev_x + sigma_t * noise \n",
    "\n",
    "    \n",
    "import random\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def sample_plot_image(eta,num,y=None):\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((1, 3, img_size, img_size), device=device)\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((1,),i, device=device, dtype=torch.long)\n",
    "        img = torch.cat((left_img.to(device),img,right_img.to(device)),dim=1)\n",
    "        img = sample_timestep(eta,img, t)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i/stepsize+1))\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "    plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872157ac-ab49-44fc-b548-c6c752fbebad",
   "metadata": {},
   "source": [
    "## Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70863e-bf26-4037-b744-74441072eade",
   "metadata": {},
   "source": [
    "### Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b02ee-0418-40bc-a116-aa97af241733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original image\n",
    "brain_img = transform(Image.open('health/brain/img/path')).unsqueeze(0)\n",
    "#tumor image\n",
    "tumor_img =transform(Image.open('tumor/img/path')).unsqueeze(0)\n",
    "#unhealth mask\n",
    "unhealth_list_mask.append(mask_transform(Image.open('data/classfire/unmask/'+tumor_files[i][:-4]+'_mask.tif')))\n",
    "\n",
    "\n",
    "#Divide the image into patches of size 256x256.\n",
    "left_img,middle_img,right_img = torch.chunk(brain_img,chunks=3,dim=3)\n",
    "#Overlay health brain images to form a nine-channel tensor.\n",
    "brain_img = torch.cat((left_img,middle_img,right_img),dim=1)\n",
    "# Add noise to the intermediate image by passing a 9-channel tensor into the function.\n",
    "img, noise = forward_diffusion_sample(brain_img, torch.tensor([L]), device)\n",
    "img_noisy = torch.cat((left_img.to(device),img.to(device),right_img.to(device)),dim=1)\n",
    "\n",
    "\n",
    "#Divide the tumor image into patches of size 256x256.\n",
    "left_tumor_img,middle_tumor_img,right_tumor_img = torch.chunk(tumor_img,chunks=3,dim=3)\n",
    "#Overlay tumoe images to form a nine-channel tensor.\n",
    "tumor_img = torch.cat((left_tumor_img,middle_tumor_img,right_tumor_img),dim=1)\n",
    "# Add noise to the intermediate image by passing a 9-channel tensor into the function.\n",
    "img_tumor, noise = forward_diffusion_sample(tumor_img, torch.tensor([L]), device)\n",
    "\n",
    "img_noisy_tumor = torch.cat((left_tumor_img.to(device),img_tumor.to(device),right_tumor_img.to(device)),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82994b91-277f-42d0-b15f-131ca4af8829",
   "metadata": {},
   "source": [
    "### Image addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf03a12-edab-4bbd-a338-6103fba8380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_unhealth = []\n",
    "mean = [0.485,0.456,0.406]\n",
    "std = [0.229,0.224,0.225]\n",
    "\n",
    "#Here, the fusion1 method is used.\n",
    "\n",
    "left_tumor_img,middle_tumor_img,right_tumor_img = torch.chunk(img_noisy_tumor,chunks=3,dim=1)\n",
    "left_img,middle_img,right_img = torch.chunk(img_noisy,chunks=3,dim=1)\n",
    "\n",
    "#To mitigate the impact of normalization, the images are inversely normalized during the fusion stage.\n",
    "left_img = left_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "right_img = right_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "middle_img = middle_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "left_tumor_img = left_tumor_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "right_tumor_img = right_tumor_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "middle_tumor_img = middle_tumor_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "res_unhealth.append(torch.cat((left_img+left_tumor_img,middle_img+middle_tumor_img,right_img+right_tumor_img),dim=0).unsqueeze(0))\n",
    "\n",
    "lef_img,middle_img,right_img = torch.chunk(res_unhealth[0],chunks=3,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c2800-7225-4030-a900-c5ebbc7711e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531307a-a9f6-4823-81a9-94dd40a17054",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Compose([\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9faf0-57ba-40b8-8868-d9d02504781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_0 = res_unhealth[i]\n",
    "\n",
    "left_img,middle_img,right_img = torch.chunk(img_0,chunks=3,dim=1)\n",
    "\n",
    "#Due to the inverse normalization applied earlier, it is necessary to renormalize the data here.\n",
    "left_img = normalize(left_img)\n",
    "right_img = normalize(right_img)\n",
    "middle_img = normalize(middle_img)\n",
    "img_0 = torch.cat((left_img,middle_img,right_img),dim=1)\n",
    "for j in range(0,L)[::-1]:\n",
    "    t = torch.full((1,), j, device=device, dtype=torch.long)\n",
    "    left_img,middle_img,right_img = torch.chunk(img_0,chunks=3,dim=1)\n",
    "    img_0 = sample_timestep(eta, img_0.to(device), t)\n",
    "    img_0 = torch.cat((left_img.to(device),img_0,right_img.to(device)),dim=1)\n",
    "mean = [0.485,0.456,0.406]\n",
    "std = [0.229,0.224,0.225]\n",
    "left_img,middle_img,right_img = torch.chunk(img_0,chunks=3,dim=1)\n",
    "\n",
    "#Inverse normalization allows the image to display properly.\n",
    "left_img = left_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "right_img = right_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "middle_img = middle_img[0]*torch.tensor(std)[:,None,None].to(device)+torch.tensor(mean)[:,None,None].to(device)\n",
    "img = trans_pil(middle_img)\n",
    "img.save('target/path')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
